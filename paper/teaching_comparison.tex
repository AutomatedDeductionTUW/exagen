In Summer 2020, all together 31 students took the online written exam
in ``Automated Deduction''. We note that in Summer 2018 and Summer
2019, there have been 17 and respectively 31 students taking
the in-class exam of the course. We believe that the
online lecturing and examination in Summer 2020 did not have negative
impact on the students' course performance.
%in 2018, there were 17 students taking the exam

In the online written examination of Summer 2020, the  students solved
their respective unique exam assignments on paper and submitted
scanned versions of their
solutions online. 
%
The types of exam problems from Summer 2020  were the same as in
previous editions of the course. 
However, contrary to previous years, different students had different exam
assignments, to minimise opportunity for collusion between students.

While building the pipeline described in this paper required much more work
than creating just one exam sheet, our approach was more efficient than
it would be to create 31~different exam sheets manually. Additionally,
our approach guaranteed that the exam problems were
unique, yet required comparable effort to solve.
Also, reusing our pipeline in the future requires only minimal changes.

Further, the types of the problems in our exam are not trivial to grade, since
the solutions require applying complicated reasoning algorithms on paper, and
the grade has to take into account the whole process, not just the result.
However, the use of templates of Section~\ref{sec:smtqf}
made the grading fairly similar to grading multiple
solutions of the same problem by providing a clear pattern to follow.
%
%  4 -- underlying argument was independent of term structure,
%       so that was easy to check,
%       what varied per student was the substitution/mgu
This observation extends to %problem 4,
the problem on non-ground superposition (Subsection~\ref{sec:fo}),
because the argument required in the solution does not depend majorly on the generated parts,
even though we did not use an explicit template.
% The same holds for problem 4, where we do not use an explicit template
% but the argument does not depend majorly on the generated parts.
%
%  1 -- more work to check as all steps may vary depending on the input;
%       (to help here, one could automate also the solution generation;
%       but: at some points there are multiple correct possibilities which affect the way forward,
%       order is not fixed,
%       and: if the student makes a mistake, we still give points for subsequent work
%           if the reasoning is correct (even if the result is not).
The situation is different for %problem 1, however.
the problem on boolean satisfiability (Subsection~\ref{sec:sat}).
There, the solution varies greatly with the input formula,
and grading a different instance requires mentally stepping through the problem again.
% It might be interesting future work to also automatically generate fully worked solutions
One might suggest to also generate fully worked solutions to this problem,
however it is not immediately clear that this would be helpful:
at various points, the students may choose among multiple correct possibilities,
each of which leads to differences in subsequent parts of the solution.

The average exam score was 79.9\,\%, compared to 80\,\% in 2019 and 76\,\%
in~2018. Based on the comparable exam averages, we believe
our online written examination from Summer 2020 did not bring any
significant change in the overall course performances of students
enrolled in the course. % sufficiently similar to the in-person examination."}

Finally, eight students filled out a feedback survey for the course in
Summer 2020. All of them reported
high levels of satisfaction with the course, with one student explicitly praising the
online exam format. Our course in Summer 2020 has been also nominated
for the {\it Best Distance Learning Award 2020} of the TU Wien. 

