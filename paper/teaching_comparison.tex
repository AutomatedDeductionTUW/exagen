This year, 30 students took the exam, compared to TODO students in 2019 (2018).
\todo{Maybe explain how students solved the problems and uploaded the solutions?}

The types of exam problems were the same as in previous years.
However, contrary to previous years, different students had different exam
assignments, to minimise opportunity for collaboration between students.
%
While building the pipeline described in this paper required much more work
than creating just one exam sheet, our approach was more efficient than
it would be to create 30~different exam sheets manually. Additionally,
our approach guaranteed that the exam problems were
unique, yet required comparable effort to solve.
Also, reusing our pipeline in the future requires only minimal changes.

Further, the types of the problems in our exam are not trivial to grade, since
the solutions require applying complicated reasoning algorithms on paper, and
the grade has to take into account the whole process, not just the result.
However, the use of templates made the grading fairly similar to grading multiple
solutions of the same problem by providing a clear pattern to follow.
\todo{How about problems 1 and 4?}

The average exam score was 79.9\%, compared to TODO in 2019 (2018).
\todo{If the scores are comparable, say something like "Thus, we believe
that the online setup was sufficiently similar to the in-person examination."}

Finally, 8 students filled out a feedback survey for the course. All of them reported
high levels of satisfaction with the course, with one student explicitly praising the
online exam format.
