Amid the COVID-19 pandemic, higher education has moved to distance
teaching. While online lecturing was relatively fast to implement via
webinars, recordings,  streaming and online communication channels,
coming up with best practices to assess course performance was far
from trivial. Even with very sophisticated technical infrastructure
(use of which, on the other hand, would be unethical to require from course
participants), avoiding collusion in the virtual
environment is very hard to achieve, if possible at all.
%In addition, course assessment is very diverse and depends on the
%available resources the  institutions have to implement individual oral
%exams or large-scale written exams.
While work on
online feedback generation has already been initiated, see
e.g.~\cite{Zuleger18,Wang18}, 
not much work on online examinations has so far emerged. 

In this paper we survey our teaching-related project work in organizing online
written exams, where the exam solutions require rigorous
logical reasoning and proofs rather than using mechanized test grids.
In particular, we are faced with the challenge of organizing online
written exams for our master's level course ``Automated
Deduction'' in logic and computation at TU
Wien\footnote{\url{https://tiss.tuwien.ac.at/course/courseDetails.xhtml?dswid=2002\&dsrid=601\&courseNr=184774\&semester=2020S}}.
This course introduces algorithmic techniques and fundamental results
in automated reasoning, by focusing on specialised algorithms for
reasoning in various fragments of first-order logics, such as
propositional logic, combinations of ground theories, and full
first-order logic with equality.
As such, topics of the course cover theoretical and practical
aspects of SAT/SMT solving~\cite{DPLL,Tinelli02,DPLLT} and first-order theorem proving using
superposition reasoning~\cite{Rubio01,Vampire13}.

We claim by no means that the framework we developed for online
examination is optimal.
Given the time constraints of examination periods, we aimed for an
online exam setting that (i) reduces collusion among students and  (ii)
requires the same workload on each participant.
The algorithmic reasoning developed within our
course called for exam sheets focused on problem solving and deductive
proofs; hence, exam sheets using test grids were not a viable solution
for written exams within our course.
We have therefore used and adapted the automated reasoning approaches introduced in our
course to automate the generation of individual exam sheets for
students enrolled in our course, by making sure that the exam tasks
remain essentially the same in each generated exam sheet. As such, we have randomly generated
individual exam problems on 
\begin{itemize}
\item
    SAT solving, by imposing (mostly) syntactical constraints on
    randomly generated SAT formulas (Section~\ref{sec:sat});
    
\item Satisfiability modulo theory (SMT) reasoning, by exploiting reasoning in a combination of theories
  and varying patterns of SMT problem templates
  (Section~\ref{sec:smt});
  
\item First-order theorem proving, by adjusting simplification
  orderings in superposition reasoning and using redundancy elimination
  in first-order proving, both in the ground/quantifier-free 
  and non-ground/quantified setting (Section~\ref{sec:fo}
  and Section~\ref{sec:qf}). 
\end{itemize}

For each of the SMT and first-order problems we generated, we used respective
SMT and first-order solvers to perform an additional sanity check
(Section~\ref{sec:implementation}).
Our toolchain and the generated benchmarks/exams are available at

\begin{center}
  \url{https://github.com/JakobR/exagen}
\end{center}

We believe our framework is beneficial not only for other
distance learning platforms, but also to researchers in automated
reasoning as we provide our scientific community
a large set of randomly generated benchmarks in SAT/SMT solving and first-order theorem proving.
While our teaching-related project delivery is specific to formal aspects of automated
reasoning, we note that our work can be extended with further
constraints to scale it to other courses in formal methods. 

This paper is structured as follows. In Sections~\ref{sec:satfo}-~\ref{sec:smtqf}
we discuss the high-level approach to generating the exam
problems. Section~\ref{sec:implementation} surveys the main implementation principles supporting our
solution.
Finally, in Section~\ref{sec:comparison} we compare
the teaching outcomes of our online written exam with those coming
from previou in-class examinations. \todo{complete the summary of comparison}
%\todo{PH: Do we want to keep the section with teaching outcomes?
%JR: I would tend towards keeping it (if we can fill the remaining
%numbers for the TODOs). LK: yes, we should keep this; good for project
%survery :)}
